{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-point, semi-honest case\n",
    "\n",
    "In this scenario, Alice and Bob are both assumed to be semi-honest, meaning that they will follow the protocol as specified, but they may try to learn more about the other party's input by analyzing the messages they receive. The goal is to compute the valuation of Bob's data point $x$, defined as the negative loss of the datapoint on Alice's model $M$.\n",
    "\n",
    "In order to achieve this, Alice and Bob will run an MPC protocol, using the library CrypTen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: The setup\n",
    "\n",
    "Before running the protocol, we need to define the model architecture (as an nn.module class) in model.py. We then need to save the model weights into `data/model.pth`, the data point $x$ into `data/data.pth` and the label into `data/lbl.pth`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9529412..0.7254902].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First, we define Alice's model M. We assume a simple CNN model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LeNet(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Adaptation of LeNet that uses ReLU activations\n",
    "    \"\"\"\n",
    "\n",
    "    # network architecture:\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.act = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "model = LeNet()\n",
    "#Save the model weights\n",
    "torch.save(model.state_dict(), 'data/model.pth')\n",
    "\n",
    "#Next, we define the data loader for CIFAR-10 dataset.\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=False,transform=transform)\n",
    "data, lbl  = trainset[4]\n",
    "classes = trainset.classes  # ['airplane', 'automobile', 'bird', ..., 'truck']\n",
    "class_name = classes[lbl]\n",
    "\n",
    "label_eye = torch.eye(10)\n",
    "label = label_eye[lbl]\n",
    "\n",
    "data = data.unsqueeze(0)  # Add batch dimension\n",
    "torch.save(data, 'data/data.pth')\n",
    "torch.save(label, 'data/lbl.pth')\n",
    "\n",
    "# Since 'image' is a Torch tensor of shape (C, H, W),\n",
    "# we need to permute the axes to (H, W, C) for display.\n",
    "plt.imshow(data[0].permute(1, 2, 0))\n",
    "\n",
    "# Add the class name as the title\n",
    "plt.title(f\"Label: {class_name}\")\n",
    "plt.axis('off')  # Hide axis ticks\n",
    "plt.show()\n",
    "\n",
    "#The variable model is Alice's model M, the variable data and lbl is Bob's contribution (x,y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Inference and Valuation\n",
    "\n",
    "Given that both parties are semi-honest, we can utilize the library CrypTen. CrypTen is an SMPC library that operates under semi-honest assumptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152309/1261688465.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./data/model.pth')) #Alice loads model weights\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "/tmp/ipykernel_152309/1261688465.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x,y = torch.load('data/data.pth'), torch.load('data/lbl.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully encrypted: True\n",
      "Valuation:  tensor(-0.2315)\n"
     ]
    }
   ],
   "source": [
    "import crypten\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "\n",
    "crypten.init()\n",
    "crypten.common.serial.register_safe_class(LeNet)\n",
    "\n",
    "#Define the rank of Alice and Bob\n",
    "ALICE = 0\n",
    "BOB = 1\n",
    "\n",
    "#Encrypt model\n",
    "model = LeNet() #Reload model \n",
    "model.load_state_dict(torch.load('./data/model.pth')) #Alice loads model weights\n",
    "dummy_input = torch.empty(data.shape)\n",
    "encrypted_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "encrypted_model.encrypt(src=ALICE)\n",
    "print(\"Model successfully encrypted:\", encrypted_model.encrypted)\n",
    "\n",
    "#Encrypt data and label\n",
    "x,y = torch.load('data/data.pth'), torch.load('data/lbl.pth')\n",
    "x = crypten.cryptensor(x)\n",
    "y = crypten.cryptensor(y)\n",
    "\n",
    "loss = crypten.nn.CrossEntropyLoss()\n",
    "\n",
    "encrypted_model.eval()\n",
    "output = encrypted_model(x)\n",
    "valuation = -loss(output, y)\n",
    "\n",
    "print(\"Valuation: \", valuation.get_plain_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in different processes\n",
    "\n",
    "To run this in different process to better simulate two different parties, we can use the `multiprocessing` library in Python. Please check and run `python launcher.py` for the full example.\n",
    "Here we benchmark the time needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:DistributedCommunicator with rank 1\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:DistributedCommunicator with rank 0\n",
      "INFO:root:==================\n",
      "INFO:root:World size = 2\n",
      "INFO:root:World size = 2\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
      "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
      "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "INFO:root:Loss value: tensor(-0.2344)\n",
      "Time taken: 3.104153633117676 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "!python launcher.py --multiprocess --world_size 2\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
