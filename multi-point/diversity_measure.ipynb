{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity measures for Data Valuation\n",
    "\n",
    "As the diversity measures concerns the whole dataset and are quite time consuming to compute, here we explore different diversity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#First, we define Alice's model M. We assume a simple CNN model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Don't use GPU for now\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "class LeNet(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Adaptation of LeNet that uses ReLU activations\n",
    "    \"\"\"\n",
    "\n",
    "    # network architecture:\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "model = LeNet()\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'data/model.pth')\n",
    "torch.save(model, 'data/alice_model.pth')\n",
    "\n",
    "#Next, we define the data loader for CIFAR-10 dataset.\n",
    "import torchvision\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=False,transform=transform,download=True)\n",
    "\n",
    "\n",
    "# Randomly select 100 images as Bob's dataset\n",
    "indices = random.sample(range(len(trainset)), 100)\n",
    "bob_images = np.array([trainset[i][0].numpy() for i in indices])\n",
    "bob_labels = np.array([trainset[i][1]  for i in indices])\n",
    "\n",
    "#Randomly select 500 images as Alice's trained dataset\n",
    "indices = random.sample(range(len(trainset)), 500)\n",
    "alice_images = np.array([trainset[i][0].numpy() for i in indices])\n",
    "alice_labels = np.array([trainset[i][1]  for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimension reduction\n",
    "\n",
    "As the diversity measures are time consuming to compute, we will first reduce the dimensionality of the dataset using random projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice receives Bob's hash:  9ffc9cdb53e8e461cdc296261d971c4a146524e00ccdbe52d679bf4cc92146e3\n",
      "Bob receives Alice's hash:  76bd7c58a60bd4296c859d32b34bf42473f3c3d130919ae464dd3a1d46824b32\n",
      "Bob receives Alice's chosen number and nonce:  3515309334 1847297474\n",
      "Alice receives Bob's chosen number and nonce:  2157137011 3876478250\n",
      "Common random number:  1360269669\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "#Secure random sharing\n",
    "bob_chosen_number = random.getrandbits(32)\n",
    "bob_nonce = random.getrandbits(32)\n",
    "bob_hashed_number = hashlib.sha256(f\"{bob_chosen_number}{bob_nonce}\".encode()).hexdigest()\n",
    "#Bob sends alice this hash\n",
    "print(\"Alice receives Bob's hash: \", bob_hashed_number)\n",
    "\n",
    "alice_chosen_number = random.getrandbits(32)\n",
    "alice_nonce = random.getrandbits(32)\n",
    "alice_hashed_number = hashlib.sha256(f\"{alice_chosen_number}{alice_nonce}\".encode()).hexdigest()\n",
    "#Alice sends bob this hash\n",
    "print(\"Bob receives Alice's hash: \", alice_hashed_number)\n",
    "\n",
    "#Alice sends Bob her chosen number and nonce\n",
    "print(\"Bob receives Alice's chosen number and nonce: \", alice_chosen_number, alice_nonce)\n",
    "#Bob verifies the hash\n",
    "assert hashlib.sha256(f\"{alice_chosen_number}{alice_nonce}\".encode()).hexdigest() == alice_hashed_number\n",
    "#Bob sends Alice his chosen number and nonceq\n",
    "print(\"Alice receives Bob's chosen number and nonce: \", bob_chosen_number, bob_nonce)\n",
    "#Alice verifies the hash\n",
    "assert hashlib.sha256(f\"{bob_chosen_number}{bob_nonce}\".encode()).hexdigest() == bob_hashed_number\n",
    "\n",
    "#Calculate common random number\n",
    "common_random_number = alice_chosen_number ^ bob_chosen_number\n",
    "print(\"Common random number: \", common_random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's images reduced shape: (100, 100)\n",
      "Alice's images reduced shape: (500, 100)\n"
     ]
    }
   ],
   "source": [
    "#In reality, Bob and Alice will independently do this on their own machines, but they have the same common random number.\n",
    "#In the malicious case, this will get a ZKP to prove that they correctly manipulated the dataset.\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "# Flatten the images\n",
    "bob_images_flat = bob_images.reshape(bob_images.shape[0], -1)\n",
    "alice_images_flat = alice_images.reshape(alice_images.shape[0], -1)\n",
    "\n",
    "# Define the random projection transformer\n",
    "n_components = 100  # You can adjust this number based on your needs\n",
    "alice_transformer = SparseRandomProjection(n_components=n_components, random_state = common_random_number)\n",
    "bob_transformer = SparseRandomProjection(n_components=n_components, random_state = common_random_number)\n",
    "\n",
    "# Fit and transform the images\n",
    "bob_images_reduced = alice_transformer.fit_transform(bob_images_flat)\n",
    "alice_images_reduced = bob_transformer.fit_transform(alice_images_flat)\n",
    "\n",
    "print(\"Bob's images reduced shape:\", bob_images_reduced.shape)\n",
    "print(\"Alice's images reduced shape:\", alice_images_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Alice's clustering\n",
    "\n",
    "Alice first performs clustering on her dataset to identify the cluster centers. This is done locally on Alice's side, and we do not protect this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers shape: (10, 100)\n"
     ]
    }
   ],
   "source": [
    "#First, we run the Kmeans clustering algorithm locally on Bob's device\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set the number of clusters\n",
    "K = 10\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=K).fit(alice_images_reduced)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "print(\"Cluster centers shape:\", cluster_centers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find points furthest to cluster centers\n",
    "\n",
    "The points that are furthest to cluster centeres are selected as the points that are least similar to the rest of Alice's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances shape: (100, 10)\n",
      "Min distances shape: (100,)\n",
      "[35.38597  31.720097] [72 34]\n"
     ]
    }
   ],
   "source": [
    "# Plaintext computation\n",
    "distances = np.linalg.norm(bob_images_reduced[:,np.newaxis,:]- cluster_centers, axis=2)\n",
    "print(\"Distances shape:\", distances.shape)\n",
    "min_distances = np.min(distances, axis=1)\n",
    "print(\"Min distances shape:\", min_distances.shape)\n",
    "sorted_indices = np.argsort(-min_distances)\n",
    "sorted_points = bob_images_reduced[sorted_indices]\n",
    "sorted_distances = min_distances[sorted_indices]\n",
    "print(sorted_distances[:2], sorted_indices[:2])\n",
    "\n",
    "#We can now use the indices to get the corresponding unreduced images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  result = load_closure(f, **kwargs)\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  result = load_closure(f, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.22799038887023926\n"
     ]
    }
   ],
   "source": [
    "#Semi-honest MPC computation\n",
    "import crypten\n",
    "import crypten.mpc as mpc\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
    "crypten.init()\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "BOB_INPUT_PATH = 'data/bob_images_reduced.pth'\n",
    "ALICE_INPUT_PATH = 'data/alice_cluster_centers.pth'\n",
    "OUTPUT_PATH = 'data/encrypted_min_distances.pth'\n",
    "\n",
    "#First we save Bob and Alice's inputs to a file\n",
    "os.makedirs('data', exist_ok=True)\n",
    "torch.save(torch.tensor(bob_images_reduced), BOB_INPUT_PATH)\n",
    "torch.save(torch.tensor(cluster_centers), ALICE_INPUT_PATH)\n",
    "\n",
    "ALICE = 0\n",
    "BOB = 1\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def secure_sort_indices():\n",
    "    # Load the inputs\n",
    "    bob_images_reduced = crypten.load_from_party(BOB_INPUT_PATH,src=BOB)\n",
    "    cluster_centers = crypten.load_from_party(ALICE_INPUT_PATH,src=ALICE)\n",
    "\n",
    "    # Compute the distances\n",
    "    diff = (bob_images_reduced.unsqueeze(1) - cluster_centers)\n",
    "    diff = diff * diff\n",
    "    pairwise_distances = diff.sum(dim=2)\n",
    "    encrypted_min_distances, _ = pairwise_distances.min(dim=1)\n",
    "    crypten.save_from_party(encrypted_min_distances.get_plain_text(),OUTPUT_PATH,src=BOB)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "secure_sort_indices()\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35.3859, 31.7200]) tensor([72, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_858366/1154103820.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  min_distances = torch.load(OUTPUT_PATH)\n"
     ]
    }
   ],
   "source": [
    "min_distances = torch.load(OUTPUT_PATH)\n",
    "sorted_indices = np.argsort(-min_distances)\n",
    "sorted_distances = min_distances.sqrt()[sorted_indices]\n",
    "print(sorted_distances[:2], sorted_indices[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Malicious scenario\n",
    "\n",
    "#Prepare Alice and Bob's private input for MPC\n",
    "Bob_input = bob_images_reduced\n",
    "Alice_input = cluster_centers\n",
    "if not os.path.exists('../MP-SPDZ/Player-Data'):\n",
    "    os.makedirs('../MP-SPDZ/Player-Data')\n",
    "p0_path = os.path.join('../MP-SPDZ/Player-Data','Input-P0-0')\n",
    "p1_path = os.path.join('../MP-SPDZ/Player-Data','Input-P1-0')\n",
    "\n",
    "#Bob as P0\n",
    "points_1d = np.array(bob_images_reduced).reshape(-1).tolist()\n",
    "with open(p0_path, 'w') as f:\n",
    "    f.write(' '.join(map(lambda x : f\"{x:.6f}\", points_1d)))\n",
    "    f.write('\\n')\n",
    "\n",
    "#Alice as P1\n",
    "points_1d = np.array(cluster_centers).reshape(-1).tolist()\n",
    "with open(p1_path, 'w') as f:\n",
    "    f.write(' '.join(map(lambda x : f\"{x:.6f}\", points_1d)))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bit length for compilation: 64\n",
      "Default security parameter for compilation: 40\n",
      "Compiling file Programs/Source/diversity.mpc\n",
      "WARNING: Order of memory instructions not preserved, errors possible\n",
      "WARNING: Probabilistic truncation leaks some information, see https://eprint.iacr.org/2024/1127 for discussion. Use 'sfix.round_nearest = True' to deactivate this for fixed-point operations.\n",
      "Writing to Programs/Bytecode/diversity-TruncPr(7)_47_16-1.bc\n",
      "Writing to Programs/Bytecode/diversity-TruncPr(6)_47_16-3.bc\n",
      "Writing to Programs/Bytecode/diversity-LTZ(40)_32-4.bc\n",
      "Writing to Programs/Bytecode/diversity-LTZ(8)_32-6.bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Programs/Bytecode/diversity-LTZ(20)_32-7.bc\n",
      "Writing to Programs/Bytecode/diversity-LTZ(4)_32-8.bc\n",
      "Writing to Programs/Schedules/diversity.sch\n",
      "Writing to Programs/Bytecode/diversity-0.bc\n",
      "Hash: 72aaeb953ee0998e776e63702522eec2ae667008ac470753c183f4577c5b6435\n",
      "Program requires at most:\n",
      "       10000 integer inputs from player 0\n",
      "        1000 integer inputs from player 1\n",
      "      133600 integer triples\n",
      "      130200 integer bits\n",
      "        1700 integer opens\n",
      "        1485 virtual machine rounds\n"
     ]
    }
   ],
   "source": [
    "#The code for valuation is prepared in ../../MP-SPDZ/Programs/Source/diversity.mpc\n",
    "# Here we compile the MPC code \n",
    "! cd ../MP-SPDZ && ./compile.py diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running /home/thomas/secure-data-valuation/MP-SPDZ/Scripts/../mascot-party.x 0 diversity -pn 19551 -h localhost -N 2\n",
      "Running /home/thomas/secure-data-valuation/MP-SPDZ/Scripts/../mascot-party.x 1 diversity -pn 19551 -h localhost -N 2\n",
      "Using statistical security parameter 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distances: [210.861, 688.783, 436.011, 816.007, 574.171, 598.343, 846.1, 674.872, 1274.02, 854.224, 1039.71, 548.749, 430.057, 600.808, 462.12, 742.737, 380.519, 1053.78, 605.01, 1065.54, 327.805, 535.211, 915.352, 799.061, 1012.09, 471.989, 179.654, 892.333, 966.648, 465.395, 508.509, 671.882, 691.021, 673.444, 1571.88, 386.224, 431.658, 607.464, 954.556, 226.936, 287.087, 762.763, 609.911, 1098.39, 805.098, 625.122, 1821.32, 673.041, 739.612, 563.078, 175.043, 302.386, 473.116, 651.197, 843.209, 1249.91, 715.023, 490.197, 970.249, 1160.19, 987.274, 247.468, 341.92, 321.873, 411.795, 1356.02, 629.365, 559.008, 543.555, 726.58, 863.188, 597.534, 1794.29, 856.321, 396.747, 346.28, 593.368, 564.044, 425.162, 633.565, 986.711, 264.749, 1346.89, 476.743, 595.748, 418.327, 393.192, 649.949, 1261.14, 523.261, 833.275, 378.959, 327.18, 548.011, 190.644, 322.045, 277.606, 405.345, 700.112, 458.565]\n",
      "The following benchmarks are including preprocessing (offline phase).\n",
      "Time = 24.1475 seconds \n",
      "Data sent = 5191.91 MB in ~9531 rounds (party 0 only; use '-v' for more details)\n",
      "Global data sent = 10383.7 MB (all parties)\n",
      "This program might benefit from some protocol options.\n",
      "Consider adding the following at the beginning of your code:\n",
      "\tprogram.use_edabit(True)\n",
      "Time taken for squared loss computation: 24.357781887054443\n"
     ]
    }
   ],
   "source": [
    "#MPC for squared loss\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "! cd ../MP-SPDZ/ && Scripts/mascot.sh diversity\n",
    "end = time.time()\n",
    "print(f\"Time taken for squared loss computation: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running /home/thomas/secure-data-valuation/MP-SPDZ/Scripts/../lowgear-party.x 0 diversity -pn 18049 -h localhost -N 2\n",
      "Running /home/thomas/secure-data-valuation/MP-SPDZ/Scripts/../lowgear-party.x 1 diversity -pn 18049 -h localhost -N 2\n",
      "Using statistical security parameter 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distances: [210.861, 688.783, 436.011, 816.007, 574.171, 598.343, 846.1, 674.872, 1274.02, 854.224, 1039.71, 548.749, 430.057, 600.808, 462.12, 742.737, 380.519, 1053.78, 605.01, 1065.54, 327.805, 535.211, 915.352, 799.061, 1012.09, 471.989, 179.654, 892.333, 966.648, 465.395, 508.509, 671.882, 691.021, 673.444, 1571.88, 386.224, 431.658, 607.464, 954.556, 226.936, 287.087, 762.763, 609.911, 1098.39, 805.098, 625.122, 1821.32, 673.041, 739.612, 563.078, 175.043, 302.386, 473.116, 651.197, 843.209, 1249.91, 715.023, 490.197, 970.249, 1160.19, 987.274, 247.468, 341.92, 321.873, 411.795, 1356.02, 629.365, 559.008, 543.555, 726.58, 863.188, 597.534, 1794.29, 856.321, 396.747, 346.28, 593.368, 564.044, 425.162, 633.565, 986.711, 264.749, 1346.89, 476.743, 595.748, 418.327, 393.192, 649.949, 1261.14, 523.261, 833.275, 378.959, 327.18, 548.011, 190.644, 322.045, 277.606, 405.345, 700.112, 458.565]\n",
      "Significant amount of unused triples of SPDZ gfp distorting the benchmark. This protocol has a large minimum batch size, which makes this unavoidable for small programs.\n",
      "The following benchmarks are including preprocessing (offline phase).\n",
      "Time = 22.1077 seconds \n",
      "Data sent = 693.236 MB in ~7293 rounds (party 0 only; use '-v' for more details)\n",
      "Global data sent = 1384.89 MB (all parties)\n",
      "This program might benefit from some protocol options.\n",
      "Consider adding the following at the beginning of your code:\n",
      "\tprogram.use_edabit(True)\n",
      "Time taken for squared loss computation: 22.320361614227295\n"
     ]
    }
   ],
   "source": [
    "#MPC for squared loss\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "! cd ../MP-SPDZ/ && Scripts/lowgear.sh diversity\n",
    "end = time.time()\n",
    "print(f\"Time taken for squared loss computation: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [210.861, 688.783, 436.011, 816.007, 574.171, 598.343, 846.1, 674.872, 1274.02, 854.224, 1039.71, 548.749, 430.057, 600.808, 462.12, 742.737, 380.519, 1053.78, 605.01, 1065.54, 327.805, 535.211, 915.352, 799.061, 1012.09, 471.989, 179.654, 892.333, 966.648, 465.395, 508.509, 671.882, 691.021, 673.444, 1571.88, 386.224, 431.658, 607.464, 954.556, 226.936, 287.087, 762.763, 609.911, 1098.39, 805.098, 625.122, 1821.32, 673.041, 739.612, 563.078, 175.043, 302.386, 473.116, 651.197, 843.209, 1249.91, 715.023, 490.197, 970.249, 1160.19, 987.274, 247.468, 341.92, 321.873, 411.795, 1356.02, 629.365, 559.008, 543.555, 726.58, 863.188, 597.534, 1794.29, 856.321, 396.747, 346.28, 593.368, 564.044, 425.162, 633.565, 986.711, 264.749, 1346.89, 476.743, 595.748, 418.327, 393.192, 649.949, 1261.14, 523.261, 833.275, 378.959, 327.18, 548.011, 190.644, 322.045, 277.606, 405.345, 700.112, 458.565]\n",
    "min_distances = np.array(answer)\n",
    "sorted_indices = np.argsort(-min_distances)\n",
    "sorted_distances = min_distances.sqrt()[sorted_indices]\n",
    "print(sorted_distances[:2], sorted_indices[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
