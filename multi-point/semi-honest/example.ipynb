{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-honest, multi-point scenario\n",
    "\n",
    "In this scenario, we assume Bob has multiple data points to contribute to Alice's ML model. Now Alice is trying to value the dataset as a whole, judging on the diversity, uncertainty of the datasets as well as the current model's performance on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: The setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#First, we define Alice's model M. We assume a simple CNN model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Don't use GPU for now\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "class LeNet(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Adaptation of LeNet that uses ReLU activations\n",
    "    \"\"\"\n",
    "\n",
    "    # network architecture:\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "model = LeNet()\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'data/model.pth')\n",
    "torch.save(model, 'data/alice_model.pth')\n",
    "\n",
    "#Next, we define the data loader for CIFAR-10 dataset.\n",
    "import torchvision\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=False,transform=transform,download=True)\n",
    "\n",
    "\n",
    "# Randomly select 100 images as Bob's dataset\n",
    "indices = random.sample(range(len(trainset)), 100)\n",
    "selected_images = np.array([trainset[i][0].numpy() for i in indices])\n",
    "selected_labels = np.array([trainset[i][1]  for i in indices])\n",
    "\n",
    "# Save images and labels separately\n",
    "# torch.save(selected_images, 'data/selected_images.pth')\n",
    "# torch.save(selected_labels, 'data/selected_labels.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Clustering\n",
    "\n",
    "Before submitting points to Alice for evaluation, Bob needs to select a subset of representative data points. To do this, we recommend using K-means clustering to select a diverse set of points where K is defined by the number of data points Alice wishs to check. Bob can select a data point closest to the centroid of each cluster. It is ultimately up to Bob to decide which points to submit, even if they are not ideal so we do not need to securely compute this step.\n",
    "\n",
    "We further make an enhancement to pure K-means selection by trying to select the most uncertain points in each cluster. As determining the uncertainty requires model inference, we define a computing budget B which is the number of points Bob and Alice can afford to evaluate. Bob can then strategically select some points in each cluster to calculate its uncertainty, and submit the points with the highest uncertainty  to Alice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers shape: (10, 3072)\n"
     ]
    }
   ],
   "source": [
    "#First, we run the Kmeans clustering algorithm locally on Bob's device\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set the number of clusters\n",
    "K = 10\n",
    "\n",
    "# Reshape the images to be a 2D array (each image is flattened)\n",
    "flattened_images = selected_images.reshape(selected_images.shape[0], -1)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(flattened_images)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "print(\"Cluster centers shape:\", cluster_centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(selected_images[0])\n",
    "torch.save(x, 'data/bob_dp.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clustering, we loop through each cluster and select the point with the highest uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
      "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  result = load_closure(f, **kwargs)\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "/home/thomas/anaconda3/envs/sdv2/lib/python3.11/site-packages/crypten/__init__.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  result = load_closure(f, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we define the uncertainty function with Crypten. \n",
    "import crypten\n",
    "import crypten.mpc as mpc\n",
    "import torch\n",
    "\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)\n",
    "ALICE = 0\n",
    "BOB = 1\n",
    "# Save the plaintext toy data\n",
    "BOB_INPUT_PATH = 'data/bob_dp.pth'\n",
    "ALICE_INPUT_PATH = 'data/alice_model.pth'\n",
    "MPC_OUTPUT_PATH = 'data/mpc_output.pth'\n",
    "INPUT_SIZE = (1,3,32,32)\n",
    "crypten.common.serial.register_safe_class(LeNet)\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def uncertainty_mpc():\n",
    "    #Load alice model\n",
    "    model = crypten.load_from_party(ALICE_INPUT_PATH, src=ALICE)\n",
    "    dummy_input = torch.empty(INPUT_SIZE)\n",
    "    private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "    private_model.encrypt(src=ALICE)\n",
    "    #Load Bob's data point\n",
    "    data = crypten.load_from_party(BOB_INPUT_PATH, src=BOB)\n",
    "    data = data.unsqueeze(0)\n",
    "    #Run model inference\n",
    "    private_model.eval()\n",
    "    result = private_model(data)\n",
    "    maxval = result.max()\n",
    "    mask_max = (result >= maxval)\n",
    "    n_max = mask_max.sum()\n",
    "    cond = n_max > 1\n",
    "    result_masked = result - (mask_max * crypten.cryptensor(2))\n",
    "    second_max_val = result_masked.max()\n",
    "    diff_raw = maxval - second_max_val\n",
    "    diff_final = -diff_raw * (crypten.cryptensor(1) - cond)\n",
    "    # result = data.get_plain_text()[0]\n",
    "    crypten.save_from_party(diff_final.get_plain_text(),MPC_OUTPUT_PATH,src=BOB)\n",
    "    # return data\n",
    "    \n",
    "uncertainty_mpc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BUDGET = 30 #This means maximum of 3 queries per cluster\n",
    "budget = TOTAL_BUDGET // K\n",
    "# Loop through each cluster\n",
    "points_to_submit = []\n",
    "for cluster_idx in range(K):\n",
    "    # Get the indices of points in the current cluster\n",
    "    cluster_points_indices = np.where(cluster_labels == cluster_idx)[0]\n",
    "    \n",
    "    # Get the points in the current cluster\n",
    "    cluster_points = flattened_images[cluster_points_indices]\n",
    "    \n",
    "    # Calculate the distance of each point to the cluster center\n",
    "    distances = np.linalg.norm(cluster_points - cluster_centers[cluster_idx], axis=1)\n",
    "    \n",
    "    # Sort the points by distance (from closest to furthest)\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    \n",
    "    used_budget = 0\n",
    "    max_uncertainty = -999\n",
    "    best_point = None\n",
    "    for idx in sorted_indices:\n",
    "        if used_budget >= budget:\n",
    "            if best_point is None:\n",
    "                best_point = cluster_points[idx] #Simply choose the point closest if no point can be queried.\n",
    "            break\n",
    "        print(f\"Point index: {cluster_points_indices[idx]}, Distance: {distances[idx]}\")\n",
    "        point = cluster_points[idx].reshape(3,32,32)\n",
    "        #Reshape the point back to input shape\n",
    "        point_tensor = torch.tensor(point)\n",
    "        torch.save(point_tensor, BOB_INPUT_PATH)\n",
    "        uncertainty_mpc()\n",
    "        answer = torch.load(MPC_OUTPUT_PATH).numpy()[0]\n",
    "        if answer > max_uncertainty:\n",
    "            max_uncertainty = answer\n",
    "            best_point = point\n",
    "        used_budget += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
