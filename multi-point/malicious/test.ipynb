{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0056], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Set up\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#First, a slightly modified version of the model \n",
    "#that returns the difference between the top 2 probs of the output.\n",
    "class LeNetZKP(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Adaptation of LeNet that uses ReLU activations\n",
    "    \"\"\"\n",
    "\n",
    "    # network architecture:\n",
    "    def __init__(self):\n",
    "        super(LeNetZKP, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.act(x)\n",
    "        top2_values, _ = torch.topk(x, 2)\n",
    "        diff = top2_values[:, 0] - top2_values[:, 1]\n",
    "        return -diff\n",
    "\n",
    "modelZK = LeNetZKP()\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "y = modelZK(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=False,transform=transform,download=True)\n",
    "\n",
    "\n",
    "# Randomly select 100 images as Bob's dataset\n",
    "indices = random.sample(range(len(trainset)), 100)\n",
    "selected_images = np.array([trainset[i][0].numpy() for i in indices])\n",
    "selected_labels = np.array([trainset[i][1]  for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying some path parameters\n",
    "model_path = os.path.join('data','network.onnx')\n",
    "compiled_model_path = os.path.join('data','network.compiled')\n",
    "pk_path = os.path.join('data','test.pk')\n",
    "vk_path = os.path.join('data','test.vk')\n",
    "settings_path = os.path.join('data','settings.json')\n",
    "\n",
    "witness_path = os.path.join('data','witness.json')\n",
    "data_path = os.path.join('data','input.json')\n",
    "output_path = os.path.join('data','output.json')\n",
    "label_path = os.path.join('data','label.json')\n",
    "\n",
    "#Model export \n",
    "torch.onnx.export(modelZK,               # model being run\n",
    "    x,                   # model input (or a tuple for multiple inputs)\n",
    "    model_path,            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=10,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensor] decomposition error: integer 611118674 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer 390866514 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -611378416 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -1821847314 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -600577992 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -974516168 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer 1971841352 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer 2446625096 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -572093377 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -2069054401 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer -3898064671 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "[tensor] decomposition error: integer 453695614 is too large to be represented by base 16384 and n 2\n",
      "forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "\n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 13, param_scale: 13, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+--------------+--------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error   | median_error | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+--------------+--------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0004504621 | 0.0005438924 | 0.0021173954 | -0.0019700825 | 0.0011665642   | 0.0005438924     | 0.0021173954  | 0.00021010637 | 0.0000018117023    | -0.00018114613     | 0.0060324976           |\n",
      "+--------------+--------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Settigns, calibration\n",
    "import ezkl\n",
    "import json \n",
    "py_run_args = ezkl.PyRunArgs()\n",
    "py_run_args.input_visibility = \"public\" #Bob can see this\n",
    "py_run_args.output_visibility = \"public\" #This hash is given to Bob\n",
    "py_run_args.param_visibility = \"private\" \n",
    "\n",
    "res = ezkl.gen_settings(model_path, settings_path, py_run_args=py_run_args)\n",
    "assert res\n",
    "\n",
    "cal_path = os.path.join('data',\"calibration.json\")\n",
    "\n",
    "#Alice should use some real data to calibrate the model, here we use random data\n",
    "data_array = (trainset.data[:10]).reshape([-1]).tolist()\n",
    "\n",
    "data = dict(input_data = [data_array])\n",
    "\n",
    "# Serialize data into file:\n",
    "json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\")\n",
    "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "assert res \n",
    "\n",
    "# srs path - This actually requires a trusted setup.\n",
    "res = await ezkl.get_srs(settings_path)\n",
    "res = ezkl.setup(\n",
    "        compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "    )\n",
    "\n",
    "assert res\n",
    "assert os.path.isfile(vk_path)\n",
    "assert os.path.isfile(pk_path)\n",
    "assert os.path.isfile(settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0034], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "point = selected_images[8]\n",
    "point_tensor = torch.tensor(point).unsqueeze(0)\n",
    "data_array = point_tensor.detach().numpy().reshape([-1]).tolist()\n",
    "data = dict(input_data = [data_array])\n",
    "json.dump(data, open(data_path, 'w'))\n",
    "modelZK.eval()\n",
    "output = modelZK(point_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "assert res\n",
    "proof_path = os.path.join('data','test.pf')\n",
    "res = ezkl.prove(witness_path, compiled_model_path, pk_path, proof_path, \"single\")\n",
    "assert res\n",
    "res = ezkl.verify(proof_path, settings_path, vk_path)\n",
    "assert res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
